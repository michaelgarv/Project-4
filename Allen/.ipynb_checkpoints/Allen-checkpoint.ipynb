{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teamID</th>\n",
       "      <th>yearID</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>H</th>\n",
       "      <th>AB</th>\n",
       "      <th>SB</th>\n",
       "      <th>HR</th>\n",
       "      <th>2B</th>\n",
       "      <th>...</th>\n",
       "      <th>ERA</th>\n",
       "      <th>RA</th>\n",
       "      <th>IPouts</th>\n",
       "      <th>HA</th>\n",
       "      <th>BBA</th>\n",
       "      <th>BAVG</th>\n",
       "      <th>SLG</th>\n",
       "      <th>H/BBA</th>\n",
       "      <th>IP</th>\n",
       "      <th>WHIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARI</td>\n",
       "      <td>2015</td>\n",
       "      <td>162</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>1494</td>\n",
       "      <td>5649</td>\n",
       "      <td>132.0</td>\n",
       "      <td>154</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>4.04</td>\n",
       "      <td>713</td>\n",
       "      <td>4400</td>\n",
       "      <td>1450</td>\n",
       "      <td>500</td>\n",
       "      <td>0.264472</td>\n",
       "      <td>0.414410</td>\n",
       "      <td>1950</td>\n",
       "      <td>1466.666667</td>\n",
       "      <td>1.329545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARI</td>\n",
       "      <td>2016</td>\n",
       "      <td>162</td>\n",
       "      <td>69</td>\n",
       "      <td>93</td>\n",
       "      <td>1479</td>\n",
       "      <td>5665</td>\n",
       "      <td>137.0</td>\n",
       "      <td>190</td>\n",
       "      <td>285</td>\n",
       "      <td>...</td>\n",
       "      <td>5.09</td>\n",
       "      <td>890</td>\n",
       "      <td>4354</td>\n",
       "      <td>1563</td>\n",
       "      <td>603</td>\n",
       "      <td>0.261077</td>\n",
       "      <td>0.431774</td>\n",
       "      <td>2166</td>\n",
       "      <td>1451.333333</td>\n",
       "      <td>1.492421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARI</td>\n",
       "      <td>2017</td>\n",
       "      <td>162</td>\n",
       "      <td>93</td>\n",
       "      <td>69</td>\n",
       "      <td>1405</td>\n",
       "      <td>5525</td>\n",
       "      <td>103.0</td>\n",
       "      <td>220</td>\n",
       "      <td>314</td>\n",
       "      <td>...</td>\n",
       "      <td>3.66</td>\n",
       "      <td>659</td>\n",
       "      <td>4323</td>\n",
       "      <td>1309</td>\n",
       "      <td>516</td>\n",
       "      <td>0.254299</td>\n",
       "      <td>0.444706</td>\n",
       "      <td>1825</td>\n",
       "      <td>1441.000000</td>\n",
       "      <td>1.266482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARI</td>\n",
       "      <td>2018</td>\n",
       "      <td>162</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>1283</td>\n",
       "      <td>5460</td>\n",
       "      <td>79.0</td>\n",
       "      <td>176</td>\n",
       "      <td>259</td>\n",
       "      <td>...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>644</td>\n",
       "      <td>4389</td>\n",
       "      <td>1313</td>\n",
       "      <td>522</td>\n",
       "      <td>0.234982</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>1835</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>1.254272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARI</td>\n",
       "      <td>2019</td>\n",
       "      <td>162</td>\n",
       "      <td>85</td>\n",
       "      <td>77</td>\n",
       "      <td>1419</td>\n",
       "      <td>5633</td>\n",
       "      <td>88.0</td>\n",
       "      <td>220</td>\n",
       "      <td>288</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>743</td>\n",
       "      <td>4395</td>\n",
       "      <td>1400</td>\n",
       "      <td>516</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.434404</td>\n",
       "      <td>1916</td>\n",
       "      <td>1465.000000</td>\n",
       "      <td>1.307850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  teamID  yearID    G   W   L     H    AB     SB   HR   2B  ...   ERA   RA  \\\n",
       "0    ARI    2015  162  79  83  1494  5649  132.0  154  289  ...  4.04  713   \n",
       "1    ARI    2016  162  69  93  1479  5665  137.0  190  285  ...  5.09  890   \n",
       "2    ARI    2017  162  93  69  1405  5525  103.0  220  314  ...  3.66  659   \n",
       "3    ARI    2018  162  82  80  1283  5460   79.0  176  259  ...  3.72  644   \n",
       "4    ARI    2019  162  85  77  1419  5633   88.0  220  288  ...  4.25  743   \n",
       "\n",
       "   IPouts    HA  BBA      BAVG       SLG  H/BBA           IP      WHIP  \n",
       "0    4400  1450  500  0.264472  0.414410   1950  1466.666667  1.329545  \n",
       "1    4354  1563  603  0.261077  0.431774   2166  1451.333333  1.492421  \n",
       "2    4323  1309  516  0.254299  0.444706   1825  1441.000000  1.266482  \n",
       "3    4389  1313  522  0.234982  0.397436   1835  1463.000000  1.254272  \n",
       "4    4395  1400  516  0.251908  0.434404   1916  1465.000000  1.307850  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "batting_df = pd.read_csv('../../Data/allDATA.csv', index_col=0)\n",
    "batting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bbaf6070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>H</th>\n",
       "      <th>AB</th>\n",
       "      <th>SB</th>\n",
       "      <th>HR</th>\n",
       "      <th>2B</th>\n",
       "      <th>3B</th>\n",
       "      <th>R</th>\n",
       "      <th>ERA</th>\n",
       "      <th>RA</th>\n",
       "      <th>IPouts</th>\n",
       "      <th>HA</th>\n",
       "      <th>BBA</th>\n",
       "      <th>BAVG</th>\n",
       "      <th>SLG</th>\n",
       "      <th>H/BBA</th>\n",
       "      <th>IP</th>\n",
       "      <th>WHIP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teamID</th>\n",
       "      <th>yearID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ARI</th>\n",
       "      <th>2015</th>\n",
       "      <td>162</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>1494</td>\n",
       "      <td>5649</td>\n",
       "      <td>132.0</td>\n",
       "      <td>154</td>\n",
       "      <td>289</td>\n",
       "      <td>48</td>\n",
       "      <td>720</td>\n",
       "      <td>4.04</td>\n",
       "      <td>713</td>\n",
       "      <td>4400</td>\n",
       "      <td>1450</td>\n",
       "      <td>500</td>\n",
       "      <td>0.264472</td>\n",
       "      <td>0.414410</td>\n",
       "      <td>1950</td>\n",
       "      <td>1466.666667</td>\n",
       "      <td>1.329545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>162</td>\n",
       "      <td>69</td>\n",
       "      <td>93</td>\n",
       "      <td>1479</td>\n",
       "      <td>5665</td>\n",
       "      <td>137.0</td>\n",
       "      <td>190</td>\n",
       "      <td>285</td>\n",
       "      <td>56</td>\n",
       "      <td>752</td>\n",
       "      <td>5.09</td>\n",
       "      <td>890</td>\n",
       "      <td>4354</td>\n",
       "      <td>1563</td>\n",
       "      <td>603</td>\n",
       "      <td>0.261077</td>\n",
       "      <td>0.431774</td>\n",
       "      <td>2166</td>\n",
       "      <td>1451.333333</td>\n",
       "      <td>1.492421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>162</td>\n",
       "      <td>93</td>\n",
       "      <td>69</td>\n",
       "      <td>1405</td>\n",
       "      <td>5525</td>\n",
       "      <td>103.0</td>\n",
       "      <td>220</td>\n",
       "      <td>314</td>\n",
       "      <td>39</td>\n",
       "      <td>812</td>\n",
       "      <td>3.66</td>\n",
       "      <td>659</td>\n",
       "      <td>4323</td>\n",
       "      <td>1309</td>\n",
       "      <td>516</td>\n",
       "      <td>0.254299</td>\n",
       "      <td>0.444706</td>\n",
       "      <td>1825</td>\n",
       "      <td>1441.000000</td>\n",
       "      <td>1.266482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>162</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>1283</td>\n",
       "      <td>5460</td>\n",
       "      <td>79.0</td>\n",
       "      <td>176</td>\n",
       "      <td>259</td>\n",
       "      <td>50</td>\n",
       "      <td>693</td>\n",
       "      <td>3.72</td>\n",
       "      <td>644</td>\n",
       "      <td>4389</td>\n",
       "      <td>1313</td>\n",
       "      <td>522</td>\n",
       "      <td>0.234982</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>1835</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>1.254272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>162</td>\n",
       "      <td>85</td>\n",
       "      <td>77</td>\n",
       "      <td>1419</td>\n",
       "      <td>5633</td>\n",
       "      <td>88.0</td>\n",
       "      <td>220</td>\n",
       "      <td>288</td>\n",
       "      <td>40</td>\n",
       "      <td>813</td>\n",
       "      <td>4.25</td>\n",
       "      <td>743</td>\n",
       "      <td>4395</td>\n",
       "      <td>1400</td>\n",
       "      <td>516</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.434404</td>\n",
       "      <td>1916</td>\n",
       "      <td>1465.000000</td>\n",
       "      <td>1.307850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">WAS</th>\n",
       "      <th>2017</th>\n",
       "      <td>162</td>\n",
       "      <td>97</td>\n",
       "      <td>65</td>\n",
       "      <td>1477</td>\n",
       "      <td>5553</td>\n",
       "      <td>108.0</td>\n",
       "      <td>215</td>\n",
       "      <td>311</td>\n",
       "      <td>31</td>\n",
       "      <td>819</td>\n",
       "      <td>3.88</td>\n",
       "      <td>672</td>\n",
       "      <td>4340</td>\n",
       "      <td>1300</td>\n",
       "      <td>495</td>\n",
       "      <td>0.265982</td>\n",
       "      <td>0.449307</td>\n",
       "      <td>1795</td>\n",
       "      <td>1446.666667</td>\n",
       "      <td>1.240783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>162</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>1402</td>\n",
       "      <td>5517</td>\n",
       "      <td>119.0</td>\n",
       "      <td>191</td>\n",
       "      <td>284</td>\n",
       "      <td>25</td>\n",
       "      <td>771</td>\n",
       "      <td>4.04</td>\n",
       "      <td>682</td>\n",
       "      <td>4338</td>\n",
       "      <td>1320</td>\n",
       "      <td>487</td>\n",
       "      <td>0.254124</td>\n",
       "      <td>0.418525</td>\n",
       "      <td>1807</td>\n",
       "      <td>1446.000000</td>\n",
       "      <td>1.249654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>162</td>\n",
       "      <td>93</td>\n",
       "      <td>69</td>\n",
       "      <td>1460</td>\n",
       "      <td>5512</td>\n",
       "      <td>116.0</td>\n",
       "      <td>231</td>\n",
       "      <td>298</td>\n",
       "      <td>27</td>\n",
       "      <td>873</td>\n",
       "      <td>4.27</td>\n",
       "      <td>724</td>\n",
       "      <td>4318</td>\n",
       "      <td>1340</td>\n",
       "      <td>517</td>\n",
       "      <td>0.264877</td>\n",
       "      <td>0.454463</td>\n",
       "      <td>1857</td>\n",
       "      <td>1439.333333</td>\n",
       "      <td>1.290181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>162</td>\n",
       "      <td>65</td>\n",
       "      <td>97</td>\n",
       "      <td>1388</td>\n",
       "      <td>5385</td>\n",
       "      <td>56.0</td>\n",
       "      <td>182</td>\n",
       "      <td>272</td>\n",
       "      <td>20</td>\n",
       "      <td>724</td>\n",
       "      <td>4.80</td>\n",
       "      <td>820</td>\n",
       "      <td>4183</td>\n",
       "      <td>1364</td>\n",
       "      <td>548</td>\n",
       "      <td>0.257753</td>\n",
       "      <td>0.417084</td>\n",
       "      <td>1912</td>\n",
       "      <td>1394.333333</td>\n",
       "      <td>1.371265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>162</td>\n",
       "      <td>55</td>\n",
       "      <td>107</td>\n",
       "      <td>1351</td>\n",
       "      <td>5434</td>\n",
       "      <td>75.0</td>\n",
       "      <td>136</td>\n",
       "      <td>252</td>\n",
       "      <td>20</td>\n",
       "      <td>603</td>\n",
       "      <td>5.00</td>\n",
       "      <td>855</td>\n",
       "      <td>4235</td>\n",
       "      <td>1469</td>\n",
       "      <td>558</td>\n",
       "      <td>0.248620</td>\n",
       "      <td>0.377438</td>\n",
       "      <td>2027</td>\n",
       "      <td>1411.666667</td>\n",
       "      <td>1.435891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 G   W    L     H    AB     SB   HR   2B  3B    R   ERA   RA  \\\n",
       "teamID yearID                                                                  \n",
       "ARI    2015    162  79   83  1494  5649  132.0  154  289  48  720  4.04  713   \n",
       "       2016    162  69   93  1479  5665  137.0  190  285  56  752  5.09  890   \n",
       "       2017    162  93   69  1405  5525  103.0  220  314  39  812  3.66  659   \n",
       "       2018    162  82   80  1283  5460   79.0  176  259  50  693  3.72  644   \n",
       "       2019    162  85   77  1419  5633   88.0  220  288  40  813  4.25  743   \n",
       "...            ...  ..  ...   ...   ...    ...  ...  ...  ..  ...   ...  ...   \n",
       "WAS    2017    162  97   65  1477  5553  108.0  215  311  31  819  3.88  672   \n",
       "       2018    162  82   80  1402  5517  119.0  191  284  25  771  4.04  682   \n",
       "       2019    162  93   69  1460  5512  116.0  231  298  27  873  4.27  724   \n",
       "       2021    162  65   97  1388  5385   56.0  182  272  20  724  4.80  820   \n",
       "       2022    162  55  107  1351  5434   75.0  136  252  20  603  5.00  855   \n",
       "\n",
       "               IPouts    HA  BBA      BAVG       SLG  H/BBA           IP  \\\n",
       "teamID yearID                                                              \n",
       "ARI    2015      4400  1450  500  0.264472  0.414410   1950  1466.666667   \n",
       "       2016      4354  1563  603  0.261077  0.431774   2166  1451.333333   \n",
       "       2017      4323  1309  516  0.254299  0.444706   1825  1441.000000   \n",
       "       2018      4389  1313  522  0.234982  0.397436   1835  1463.000000   \n",
       "       2019      4395  1400  516  0.251908  0.434404   1916  1465.000000   \n",
       "...               ...   ...  ...       ...       ...    ...          ...   \n",
       "WAS    2017      4340  1300  495  0.265982  0.449307   1795  1446.666667   \n",
       "       2018      4338  1320  487  0.254124  0.418525   1807  1446.000000   \n",
       "       2019      4318  1340  517  0.264877  0.454463   1857  1439.333333   \n",
       "       2021      4183  1364  548  0.257753  0.417084   1912  1394.333333   \n",
       "       2022      4235  1469  558  0.248620  0.377438   2027  1411.666667   \n",
       "\n",
       "                   WHIP  \n",
       "teamID yearID            \n",
       "ARI    2015    1.329545  \n",
       "       2016    1.492421  \n",
       "       2017    1.266482  \n",
       "       2018    1.254272  \n",
       "       2019    1.307850  \n",
       "...                 ...  \n",
       "WAS    2017    1.240783  \n",
       "       2018    1.249654  \n",
       "       2019    1.290181  \n",
       "       2021    1.371265  \n",
       "       2022    1.435891  \n",
       "\n",
       "[210 rows x 20 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_batting_df = batting_df.set_index(['teamID', 'yearID'])\n",
    "grouped_batting_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0574dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "X = grouped_batting_df[[\"SLG\", \"SB\", \"BAVG\"]].values\n",
    "y = grouped_batting_df[[\"R\"]].values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fd255e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d68371b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 8)                 32        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_31 (Dense)            (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83 (332.00 Byte)\n",
      "Trainable params: 83 (332.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "44772ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3063adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 2ms/step - loss: -485.0912 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -514.5847 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -544.1679 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -573.1910 - accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -602.9333 - accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -634.8027 - accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -666.6823 - accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -700.9481 - accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -735.8052 - accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -772.2635 - accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: -809.2141 - accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -850.7004 - accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -892.7795 - accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -936.1747 - accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -982.1954 - accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1029.8568 - accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1081.7179 - accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1135.1084 - accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1190.5580 - accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1249.1727 - accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -1309.6343 - accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1373.1487 - accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1441.7450 - accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -1510.7971 - accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -1587.9570 - accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -1661.0957 - accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1741.7166 - accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1826.8667 - accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -1912.5114 - accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -2002.6467 - accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -2097.6309 - accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: -2193.1130 - accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -2295.2595 - accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -2398.8271 - accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -2507.0742 - accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -2621.2810 - accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -2736.6907 - accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -2856.6514 - accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -2987.9475 - accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -3111.1418 - accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -3247.9768 - accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -3389.4614 - accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -3532.5437 - accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -3679.9976 - accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -3835.7222 - accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -3998.5784 - accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -4160.2061 - accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -4334.6646 - accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -4508.7021 - accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -4693.4922 - accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -4876.5601 - accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -5069.0908 - accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -5270.9492 - accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: -5480.3130 - accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -5691.2119 - accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -5908.7271 - accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -6131.9473 - accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -6365.5918 - accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -6600.9976 - accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -6850.3975 - accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -7096.8936 - accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -7351.3638 - accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -7626.7866 - accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: -7895.2856 - accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -8179.8584 - accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -8474.4785 - accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -8764.7441 - accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -9075.4395 - accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -9386.6797 - accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -9711.8945 - accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -10033.7090 - accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -10377.3760 - accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -10724.8379 - accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: -11074.9023 - accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -11443.2607 - accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -11833.5000 - accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -12212.6748 - accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -12599.8574 - accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -13009.4365 - accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -13426.3818 - accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -13850.5146 - accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -14288.1924 - accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -14725.7578 - accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -15190.0176 - accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -15660.5322 - accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -16145.3945 - accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -16627.0820 - accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -17137.8281 - accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -17650.6719 - accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -18186.3750 - accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -18713.4336 - accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -19278.4805 - accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -19835.7949 - accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -20414.7012 - accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -20998.8320 - accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -21602.2695 - accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -22212.5938 - accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -22838.3828 - accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: -23485.5898 - accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -24136.2285 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2b875001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000001F1063969D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 - 0s - loss: -2.5775e+04 - accuracy: 0.0000e+00 - 97ms/epoch - 48ms/step\n",
      "Loss: -25775.087890625, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
